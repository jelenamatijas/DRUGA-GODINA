{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c594bce-8cb5-4396-af1a-1ccbae279469",
   "metadata": {},
   "source": [
    "## Logistička regresija\n",
    "\n",
    "Logistička regresija predstavlja algoritam za binarnu klasifikaciju zasnovan na statistici. Iako je veoma jednostavan i nema veliku upotrebu danas, algoritam logističk regresije uvodi neke ideje koje su nam bitne za naredna predavanja. \n",
    "\n",
    "### Sigmoid funkcija\n",
    "\n",
    "Spominjali smo prije da regresija podrazumijeva predviđanje realne vrijednosti. Isto tako i u logističkoj regresiji prvo predviđamo realnu vrijednost upotrebom formule za lineranu regresiju:\n",
    "\n",
    "$$\n",
    "Y = w * X + b\n",
    "$$\n",
    "\n",
    "Pošto sada radimo sa klasifikacijom potrebno ja da datu realnu vrijednost na neki način pretvorimo u klasu. Za pretvaranje realne vrijednosti u klasu koristimo sigmoid funkciju. Sigmodi funkcija je definisana sljedećom formulom:\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e ^{-x}}\n",
    "$$\n",
    "\n",
    "Grafik funkcije se može vidjeti na prezentaciji. Ono što je bitno je da funkcija sigmoid sabija ulaz u segment $[0, 1]$, tako da negativne realne vrijednosti teže 0 dok pozitivne vrijednosti teže 1. Sa aspekta klasifikacije izlaz funkcije sigmoid posmatramo kao vjerovatnoću da da izlaz pripada klasi 1, odnosno ako je izlaz sigmoid funkcije ispod 0.5 ulaz ćemo klasifikovati u klasu 0, a u suprotnom u klasu 1. Koristeći sigmoid funkciju možemo reći da je model logističke regresije zasnovan na sljedećoj formuli:\n",
    "$$\n",
    "    P(y=1 | x) = \\sigma(w*x + b)\n",
    "$$\n",
    "\n",
    "### Funkcija greške\n",
    "\n",
    "Drugi bitan aspekt na koji moramo da se osvrnemo je funkcija greške. Kod standardne linearne regresije minimizovali smo kvadrat razlike. Međutim u slučaju logističke regresije naš cilj nije minimizacija razlika predviđene realne vrijednosti i očekivane, već adekvatna klasifikacija ulaza. Osnovna ideja je da za ulaze koji pripadaju klasi 1, želimo da maksimizujemo izlaz sigmoid funkcije, odnosno da maksimizujemo vjerovatnoću koju naš model predviđa da ulaz pripada klasi 1. Dati metod se zove Maximum Likelihood Estimation (MLE) i zasnovan je na sljedećoj formuli.\n",
    "\n",
    "$$\n",
    "L(w, b) =  \\prod{[P(y=1 | x)*y + (1 - P(y=1 | x))*(1-y)]}\n",
    "$$\n",
    "\n",
    "Možemo primijetiti da u slučaju kad je stvarna klasa 1, $y = 1$, posmatra se samo prvi dio formule, odnosno želimo da izlaz sigmoid funkcije bude što veći. U durgom slučaju kad je klasa 0, $y = 0$, posmatramo samo drugi član formule, odnosno želimo da nam izlaz sigmoid funkcije bude što manji. MLE možemo intuitivno posmatrati kao mjeru koliko su naše vjerovatnoće blizu stvarnim klasama. Često se zbog numeričke stabilnosti i lakšeg računanja koristi logartiam MLE formule odnosno:\n",
    "$$\n",
    "    LL(w, b) = \\sum{[y * log(P(y=1 | x)) + (1-y) * log(1 - P(y=1 | x))]}\n",
    "$$\n",
    "\n",
    "Rekli smo da je ideja da želimo da maksimizujemo vjerovatnoću, ali u slučaju algoritama mašinskog učenja osnovni koncept je minimizacija greške. Zbog toka umjesto maksimizacije upotrebom prethodne formule $LL(w, b)$, vršimo minimizaciju upotrebom $-LL(w, b)$.\n",
    "\n",
    "### Gradient descent\n",
    "\n",
    "Nakon što smo izračunali grešku potrebno je da promijenimo naše težine w i b, kako bi se greška smanjila. S obizirom da znamo funkciju greške u zavisnosti od w i b, možemo potencijalno kao u slučaju linearne regresije da nađemo prvi izvod funkcije po w i b, pa da pronađemo njegove 0 kako bismo našli globalni minimum. Dati će dati najbolje rezultate međutim u praksi često nije izvodljiv zbog veoma kompleksnih zavisnosti i velikog broja parametara. Pored toga ne postoje programski okviri koji podržavaju tačno rješavanje datog problema, što bi značilo da za svaki problem moramo prvo ručno da raspišemo račun na osnovu kog bi kucali kod. Kako bismo rješili da ti problem u mašinskom učenju koristi se tehnika gradijentog spusta, koja samo zahtijeva da znamo koja je formula za prvi izvod neke funkcije ali ne uključuje računanje njegovih nula (što predstavlja veći problem). Osnovna ideja gradijetnog spusta je zasnovana na kretanju u suprtonom pravu od gradijenta funkcija. Uzmimo u obzir da prvi izvod funkcije definiše kada je ona rastuća ili opadajuća. Ako izračunarmo prvi izvod funkcije greške za neku kombinaciju parametara w i b, mi bismo željeli da se krećemo u pravcu suprotnom od rasta funkcije, zato što želimo da minimizujemo grešku. Želimo da mijenjamo naše parametre tako da se funkcija kreće u pravcu opadanja.\n",
    "\n",
    "Koraci za gradijentni spust:\n",
    "1. Inicijalizujemo model sa nasumičnim parametrima w i b\n",
    "2. Izračunamo vrijednost gradijenta funkcije greške za date parametre\n",
    "3. Promijenimo parametre tako da se krećemo u pravcu suprotnom od gradijenta\n",
    "4. Ponavljamo korake 2 i 3 određeni broj iteracija\n",
    "\n",
    "Funkcija promejene parametara $w$ i $b$ je definisana na sljedeći način:\n",
    "$$\n",
    "    w = w - \\alpha * \\frac{\\partial LL(w, b)}{\\partial w}\n",
    "    b = b - \\alpha * \\frac{\\partial LL(w, b)}{\\partial b}\n",
    "$$\n",
    "Faktor $\\alpha$ se naziva faktorom obučavanja i definiše koliko brzo ćemo mijenjati vrijednost $w$ i $b$ u svakoj iteraciji. Ukoliko je faktor obučavanja previše malen promjene će biti jako spore pa samim tim i konvergencija kao minimumu će zahtijevati veliki broj iteracija. Takođe, sa druge strane ukoliko je $\\alpha$ preveliko možemo imati slučaj da stalno preskačemo minimum zbog prevelikih promjena (pogledati prezentaciju).\n",
    "\n",
    "Veoma je bitno napomenuti da tehnikom gradijentnog spusta nikad nećemo dobiti idealne rezultate i da nikad nećemo pronaći pravi globalni minimum. Ipak nakon dovoljno iteracija sa adekvatnim faktorom obučavanja možemo prići dovoljno blizu minimumu, tako da naš model ima prihvatljive performanse. \n",
    "\n",
    "#### Zadatak za vježbu\n",
    "\n",
    "Izvesti formule za $\\frac{\\partial LL(w, b)}{\\partial w}$ i $\\frac{\\partial LL(w, b)}{\\partial b}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55f36698-a621-4b6a-899e-6199cb43d0b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "737992ca-a504-4d2d-b324-edd373b6b038",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "class LogisticRegressionCustom(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Parametri\n",
    "    ---------\n",
    "    learning_rate : float, default=0.01\n",
    "        Veličina koraka prilikom gradijentnog spusta.\n",
    "    num_iterations : int, default=1000\n",
    "        Broj iteracija za optimizaciju.\n",
    "    threshold : float, default=0.5\n",
    "        Prag za klasifikaciju (pretvaranje verovatnoće u labelu).\n",
    "    verbose : bool, default=False\n",
    "        Ako je True, ispisuje se gubitak svakih 100 iteracija.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000, threshold=0.5, verbose=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.threshold = threshold\n",
    "        self.verbose = verbose\n",
    "        # Atributi koji se postavljaju tokom fitovanja\n",
    "        self.w_ = None\n",
    "        self.b_ = None\n",
    "        self.loss_history_ = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Pretvaranje ulaza u numpy niz i reshaping ciljne promenljive\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y).reshape(-1, 1)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Inicijalizacija parametara\n",
    "        self.w_ = np.zeros((n_features, 1))\n",
    "        self.b_ = 0\n",
    "\n",
    "        # Gradijentni spust\n",
    "        for i in range(self.num_iterations):\n",
    "            z = X.dot(self.w_) + self.b_\n",
    "            a = sigmoid(z)\n",
    "\n",
    "            # Računanje gradijenata\n",
    "            dw = (1 / n_samples) * X.T.dot(a - y)\n",
    "            db = (1 / n_samples) * np.sum(a - y)\n",
    "\n",
    "            # Ažuriranje parametara\n",
    "            self.w_ -= self.learning_rate * dw\n",
    "            self.b_ -= self.learning_rate * db\n",
    "\n",
    "            # Praćenje gubitka\n",
    "            if self.verbose and i % 100 == 0:\n",
    "                loss = - (1 / n_samples) * np.sum(y * np.log(a) + (1 - y) * np.log(1 - a))\n",
    "                self.loss_history_.append(loss)\n",
    "                print(f\"Gubitak nakon iteracije {i}: {loss}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        # Izračunava verovatnoće za klase 0 i 1\n",
    "        X = np.asarray(X)\n",
    "        z = X.dot(self.w_) + self.b_\n",
    "        probs = sigmoid(z)\n",
    "        return np.hstack([1 - probs, probs])\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Pretvara verovatnoću klase 1 u binarnu predikciju\n",
    "        probs = self.predict_proba(X)[:, 1]\n",
    "        return (probs >= self.threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "236a4014-54b0-43b5-a456-2702e61d0274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gubitak nakon iteracije 0: 0.6931471805599452\n",
      "Gubitak nakon iteracije 100: nan\n",
      "Gubitak nakon iteracije 200: nan\n",
      "Gubitak nakon iteracije 300: nan\n",
      "Gubitak nakon iteracije 400: nan\n",
      "Gubitak nakon iteracije 500: nan\n",
      "Gubitak nakon iteracije 600: nan\n",
      "Gubitak nakon iteracije 700: nan\n",
      "Gubitak nakon iteracije 800: nan\n",
      "Gubitak nakon iteracije 900: nan\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.97      0.91        63\n",
      "           1       0.98      0.91      0.94       108\n",
      "\n",
      "    accuracy                           0.93       171\n",
      "   macro avg       0.92      0.94      0.93       171\n",
      "weighted avg       0.94      0.93      0.93       171\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10151/1357729101.py:6: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/tmp/ipykernel_10151/1357729101.py:58: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = - (1 / n_samples) * np.sum(y * np.log(a) + (1 - y) * np.log(1 - a))\n",
      "/tmp/ipykernel_10151/1357729101.py:58: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = - (1 / n_samples) * np.sum(y * np.log(a) + (1 - y) * np.log(1 - a))\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegressionCustom(learning_rate=0.01, num_iterations=1000, verbose=True)\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306e81c9-2af1-46f3-8ca5-986357b6439b",
   "metadata": {},
   "source": [
    "## LightGBM - staro gradivo (nije obavezno, zanimljivo koga zanima više)\n",
    "\n",
    "LigthGBM je Microsoft-ov algoritam za rad sa tabularnim (podaci koji se mogu predstaviti u vidu tabele) podacima zasnovan na stablima odlučivanja. Uvodi brojne optimizacije nad standardnim stablima odlučivanja što mu daje veliku efikasnot i stabilnost, te omogućava da dostigne veoma dobre performanse na velikom broju problema. Zasnovan je na ideji Boosting-a o kojoj smo pričali na prošlom času, odnosno trenira se veći broj weak learner-a čiji je cilj da isprave greške prethodnika. Da bismo instalirali LightGBM u pythonu, koristimo komandu:\n",
    "```bash\n",
    "    pip install lightgbm\n",
    "```\n",
    "\n",
    "### Gradient Boosting i GOSS\n",
    "\n",
    "Za računanje splitova i optimizaciju stabla LightGBM koristi varijantu Boosting algoritma zasnovanu na gradient descent-u (GBDT). Zbog veće računske kompleksnosti nećemo ulaziti u tačan račun, ali zainteresovani mogu pročitati originalni rad ako žele: https://proceedings.neurips.cc/paper_files/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf. Dodatna optimizacija koju LightGBM uvodi prilikom računanja gradienta je fokusiranje na ulaze koji imaju visoku grešku. Data tehnika se naziva Gradient-based One-Side Sampling (GOSS) i podrazumijeva da se tokom računanja koriste samo ulazi sa visokom greškom, dok se ostali ignorišu. Data optimimizacija značajno ubrzava rad algoritma i omogućava mu da ispuni onaj Boosting aspekat, odnosno fokus na greške prethodnog modela.\n",
    "\n",
    "### Leaf wise vs Level wise growth\n",
    "\n",
    "Druga optimizacija koju LigthGBM uvodi je Leaf wise growth. Većina standardnih algoritama koji rade sa stablima odlučivanja koriste Level wise growth. Level wise growth podrazumijeva da na pri svakoj iteraciji prođemo kroz sve čvorove na trenutnom nivou i nađemo novi uslov grananja za svaki od njih. Takav pristup može značajno da poveća šanse za overfitting (da li ima smisla da radimo split na već homogenom podskupu), a i značajno usporava rad algoritma pri svakoj novoj iteraciji. Leaf wise growth podrazumijeva da u svakoj iteraciji pronađemo čvoj koji ima najveću grešku, pa da za dati čvor tražimo novi uslov grananja koji bi potencijalno mogao da smanji datu grešku. Na taj način širimo naše stablo i uvodimo nove uslove grananja samo na mjestima gdje je to potrebno.\n",
    "\n",
    "### EFB (Exclusive Feature Bundling)\n",
    "\n",
    "Posljednja optimizacija koju LightGBM uvodi odnosi se na redukciju dimenzionalnosti našeg ulaza. Rekli smo da pri radu sa velikim skupovima podataka pored velikog broja ulaza često imamo i problem da svaki ulaz ima i veliki broj kolona. Kako bismo ubrzali rad algoritma veoma je bitno da nađemo načine da redukujemo broj kolona (featurea). EFB predstavlja tehniku koja se zasniva na ideji da su u praksi često neke kolone sparse vektori, odnosno da imaju veliki broj nula. Ukoliko se u datasetu nalazi više takvih kolona koje nemaju u isto vrijeme ne nula vrijednosti EFB će takve kolone spojiti u jednu kolonu. EFB značajno ubrzava rad i smanjuje upotrebu memorije."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7116a909-1625-4df7-9bbd-374801f90f5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00\n",
      " 2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "print(X[0])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b6c5cf4-1b35-4cc6-ab09-2a55088c1f6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        14\n",
      "           1       1.00      1.00      1.00        14\n",
      "           2       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           1.00        36\n",
      "   macro avg       1.00      1.00      1.00        36\n",
      "weighted avg       1.00      1.00      1.00        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# kreiramo klasifikator na bazi lightgbm-a\n",
    "lgbm_clf = lgb.LGBMClassifier()\n",
    "\n",
    "# isti interfejs za trening kao i prije\n",
    "lgbm_clf.fit(X_train, y_train)\n",
    "\n",
    "# isti interfejs za inferenciju\n",
    "y_pred = lgbm_clf.predict(X_test)\n",
    "\n",
    "# classification report funkcija vraca dictionary svih bitnih metrika\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b8b490-cc6d-484d-96bf-f0566fefde75",
   "metadata": {},
   "source": [
    "## Pretraga hiperparametara\n",
    "\n",
    "Većina ozbiljnijih algoritama poput LightGBM-a ima veliki broj parametara koje treba isprobati da bismo vidjeli koja kombinacija daje najbolje performanse na našem skupu podataka. Zbog toga bitno je da imamo jasno definisane korake koji nam mogu dati informacije o tome koja je kombinacija najbolja. Danas ćemo obraditi dva pristupa pretrage hiperparametara.\n",
    "\n",
    "### Validacioni skup podataka\n",
    "\n",
    "Prvi pristup podrazumijeva kreiranje novog skupa podataka koji se zove validacioni skup i koji se koristi za validaciju određene kombinacije parametara. Validacioni skup sadrži ulaze koji se ne nalaze ni u trening ni u testnom skupu. Obično se kreira tako što se za testni skup uzme veći procenat ulaza, pa se onda testni skup dodatno podijeli na dva jednaka dijela. Pored pretrage hiperparametara validacioni skup se koristi da spriječi overfitting, odnosno za tehniku ranog zaustavljanja o kojoj smo diskutovali prošli put. Ukoliko imamo validacioni skup algoritam se izvršava sve dok njegove performanse ne počnu da se pogoršavaju na validacionom skupu (pogledati prezentaciju za vizualizaciju).\n",
    "\n",
    "#### Proces pretrage hiperparametara\n",
    "\n",
    "Pri radu sa validacionim skupom uvijek imamo isti skup koraka koje koristimo za pretragu hiperparametara:\n",
    "1. Definišemo moguće varijacije parametara - skup parova (parametar algoritma, lista vrijednosti koje želimo isprobati).\n",
    "2. Za svaku moguću kombinaciju treniramo algoritam upotrebom trening skupa. Pri svakoj iteraciji provjeravamo kakve su performanse algoritma na validacionom skupu (recimo kakva je neka od metrika precission ili recall). Prekidamo trening kada performanse na validacionom skupu počnu da opadaju.\n",
    "3. Pamtimo kakve su bile performanse algoritma na validacionom skupu\n",
    "4. Biramo kombinaciju parameteara koja daje najbolje rezultate\n",
    "5. Dodatno treniramo model sa najboljim parametrima i testiramo njegove rezultate na testnom skupu kako bismo dobili konačne performanse\n",
    "\n",
    "#### Zašto novi skup?\n",
    "\n",
    "Veoma je bitno da se testni skup ne koristi kao validacioni skup. Glavni razlog za to je što želimo da testnim skupom emuliramo rad algoritma u realnom okruženju odnosno da vidimo kako će raditi sa ulazima koje nikad prije nije vidio. Ukoliko koristimo testni skup kao validacioni može se desiti da će algoritam početi u nekoj mjeri da optimizuje u odnosu na testni skup, čime više naši konačni rezultati više ne predstavljaju realnu sliku, jer je vršena optimizacija u odnosu na testni skup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e763330-8401-478b-9a43-00e3fbb70401",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 1.0\n",
      "Best parameters: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96        26\n",
      "           1       0.98      0.98      0.98        60\n",
      "\n",
      "    accuracy                           0.98        86\n",
      "   macro avg       0.97      0.97      0.97        86\n",
      "weighted avg       0.98      0.98      0.98        86\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, classification_report\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "class ManualParameterGrid:\n",
    "    def __init__(self, param_grid):\n",
    "\n",
    "        self.param_grid = param_grid\n",
    "        self._keys = list(param_grid.keys())\n",
    "        self._values = [param_grid[k] for k in self._keys]\n",
    "        total = 1\n",
    "        for vals in self._values:\n",
    "            if not hasattr(vals, '__len__'):\n",
    "                raise ValueError(f\"Value for {k!r} is not iterable\")\n",
    "            total *= len(vals)\n",
    "        self._length = total\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def __iter__(self):\n",
    "        for combo in itertools.product(*self._values):\n",
    "            yield dict(zip(self._keys, combo))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < 0:\n",
    "            idx += self._length\n",
    "        if not (0 <= idx < self._length):\n",
    "            raise IndexError(f\"Index {idx} out of range\")\n",
    "        sizes = [len(v) for v in self._values]\n",
    "        combo = {}\n",
    "        for key, vals, size in zip(self._keys, self._values, sizes):\n",
    "            idx, rem = divmod(idx, np.prod(sizes[sizes.index(size)+1:]) if size != sizes[-1] else idx)\n",
    "            combo[key] = vals[idx]\n",
    "            sizes.pop(0)\n",
    "        return combo\n",
    "\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "X, y = breast_cancer.data, breast_cancer.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'min_samples_split': [2, 4, 8],\n",
    "}\n",
    "\n",
    "manual_grid = ManualParameterGrid(param_grid)\n",
    "\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for params in manual_grid:\n",
    "    clf = DecisionTreeClassifier(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_val)\n",
    "    score = recall_score(y_val, y_pred)\n",
    "    if score > best_score:\n",
    "        best_score, best_params = score, params\n",
    "\n",
    "print(\"Best score:\", best_score)\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n",
    "best_clf = DecisionTreeClassifier(**best_params)\n",
    "best_clf.fit(X_train, y_train)\n",
    "y_pred = best_clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b58aabe-4485-43b2-8ef3-e4577a4d6dd6",
   "metadata": {},
   "source": [
    "### Unkarsna validacija (Cross Validation)\n",
    "\n",
    "Glavni problem upotrebe validacionog skupa je što dodatno oduzima podatke iz našeg trening skupa. Ukoliko imamo veliki skup podataka to neće predstavljati problem, međutim često imamo probleme gdje nemamo veliki broj podataka na raspolaganju. U takvim slučajevima svaka trening instanca nam je bitna i oduzimanje od trening skupa može dovesti do situacije da trening skup ne reprezentuje adekvatno realno stanje našeg problema što onda dovodi do lošeg modela. Unakrsa validacija predstavlja drugi pristup validaciji koji ne zahtijeva kreiranje novog skupa podataka. Kod unakrsne validacije trening skup se dijeli u k jednakih dijelova (K folds). Nakon toga za svaku kombinaciju parametara model se trenira sa k-1 dijelova i validira se sa preostalim dijelom. Dati proces se ponavlja za trenugnu kombinaciju hiperparametara sve dok ne izvršimo trening i validaciju za svaku kombinaciju k foldova (odnosno sve dok svaki od foldova ne bude validacioni skup). Konačne performanse za datu kombinaciju parametara dobijamo tako što nađemo srednju vrijednost za svaki fold. U ovom slučaju nemamo gubitak podataka ali imamo povećanu računsku kompelksnost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "458120bc-69f8-4e27-8a15-9b1dc4382cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "class ManualGridSearchCV:\n",
    "    def __init__(self, estimator, param_grid, scoring='neg_mean_squared_error', cv=5, verbose=0, random_state=42):\n",
    "        if not isinstance(cv, int) or cv < 2:\n",
    "            raise ValueError(\"cv must be an integer >= 2\")\n",
    "        self.estimator = estimator\n",
    "        self.param_grid = ManualParameterGrid(param_grid)\n",
    "        self.scoring = scoring\n",
    "        self.cv_folds = cv\n",
    "        self.verbose = verbose\n",
    "        self.random_state = random_state\n",
    "        self._rng = np.random.RandomState(self.random_state)\n",
    "\n",
    "    def _score(self, y_true, y_pred):\n",
    "        return self.scoring(y_true, y_pred)\n",
    "\n",
    "    def _manual_kfold(self, n_samples):\n",
    "        indices = np.arange(n_samples)\n",
    "        self._rng.shuffle(indices)\n",
    "        fold_sizes = np.full(self.cv_folds, n_samples // self.cv_folds, dtype=int)\n",
    "        fold_sizes[:n_samples % self.cv_folds] += 1\n",
    "\n",
    "        current = 0\n",
    "        for fold_size in fold_sizes:\n",
    "            start, stop = current, current + fold_size\n",
    "            val_idx = indices[start:stop]\n",
    "            train_idx = np.concatenate([indices[:start], indices[stop:]])\n",
    "            yield train_idx, val_idx\n",
    "            current = stop\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        best_score = -np.inf\n",
    "        best_params = None\n",
    "        self.cv_results_ = []\n",
    "\n",
    "        for idx, params in enumerate(self.param_grid):\n",
    "            if self.verbose > 1:\n",
    "                print(f\"[{idx+1}/{len(self.param_grid)}] Testing {params}\")\n",
    "            fold_scores = []\n",
    "\n",
    "            for train_idx, val_idx in self._manual_kfold(n_samples):\n",
    "                X_tr, X_val = X[train_idx], X[val_idx]\n",
    "                y_tr, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "                model = clone(self.estimator).set_params(**params)\n",
    "                model.fit(X_tr, y_tr)\n",
    "                y_pred = model.predict(X_val)\n",
    "                fold_scores.append(self._score(y_val, y_pred))\n",
    "\n",
    "            mean_score = np.mean(fold_scores)\n",
    "            self.cv_results_.append({'params': params, 'mean_test_score': mean_score})\n",
    "\n",
    "            if mean_score > best_score:\n",
    "                best_score, best_params = mean_score, params\n",
    "\n",
    "        self.best_score_ = best_score\n",
    "        self.best_params_ = best_params\n",
    "        self.best_estimator_ = clone(self.estimator).set_params(**best_params)\n",
    "        self.best_estimator_.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.best_estimator_.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93b33add-afa9-4904-9da4-90f0ff57ec77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/144] Testing {'criterion': 'squared_error', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "[2/144] Testing {'criterion': 'squared_error', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 4}\n",
      "[3/144] Testing {'criterion': 'squared_error', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 8}\n",
      "[4/144] Testing {'criterion': 'squared_error', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "[5/144] Testing {'criterion': 'squared_error', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 4}\n",
      "[6/144] Testing {'criterion': 'squared_error', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 8}\n",
      "[7/144] Testing {'criterion': 'squared_error', 'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "[8/144] Testing {'criterion': 'squared_error', 'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 4}\n",
      "[9/144] Testing {'criterion': 'squared_error', 'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 8}\n",
      "[10/144] Testing {'criterion': 'squared_error', 'max_depth': None, 'min_samples_leaf': 8, 'min_samples_split': 2}\n",
      "[11/144] Testing {'criterion': 'squared_error', 'max_depth': None, 'min_samples_leaf': 8, 'min_samples_split': 4}\n",
      "[12/144] Testing {'criterion': 'squared_error', 'max_depth': None, 'min_samples_leaf': 8, 'min_samples_split': 8}\n",
      "[13/144] Testing {'criterion': 'squared_error', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "[14/144] Testing {'criterion': 'squared_error', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 4}\n",
      "[15/144] Testing {'criterion': 'squared_error', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 8}\n",
      "[16/144] Testing {'criterion': 'squared_error', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "[17/144] Testing {'criterion': 'squared_error', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 4}\n",
      "[18/144] Testing {'criterion': 'squared_error', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 8}\n",
      "[19/144] Testing {'criterion': 'squared_error', 'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "[20/144] Testing {'criterion': 'squared_error', 'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 4}\n",
      "[21/144] Testing {'criterion': 'squared_error', 'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 8}\n",
      "[22/144] Testing {'criterion': 'squared_error', 'max_depth': 5, 'min_samples_leaf': 8, 'min_samples_split': 2}\n",
      "[23/144] Testing {'criterion': 'squared_error', 'max_depth': 5, 'min_samples_leaf': 8, 'min_samples_split': 4}\n",
      "[24/144] Testing {'criterion': 'squared_error', 'max_depth': 5, 'min_samples_leaf': 8, 'min_samples_split': 8}\n",
      "[25/144] Testing {'criterion': 'squared_error', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "[26/144] Testing {'criterion': 'squared_error', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 4}\n",
      "[27/144] Testing {'criterion': 'squared_error', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 8}\n",
      "[28/144] Testing {'criterion': 'squared_error', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "[29/144] Testing {'criterion': 'squared_error', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 4}\n",
      "[30/144] Testing {'criterion': 'squared_error', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 8}\n",
      "[31/144] Testing {'criterion': 'squared_error', 'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "[32/144] Testing {'criterion': 'squared_error', 'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 4}\n",
      "[33/144] Testing {'criterion': 'squared_error', 'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 8}\n",
      "[34/144] Testing {'criterion': 'squared_error', 'max_depth': 10, 'min_samples_leaf': 8, 'min_samples_split': 2}\n",
      "[35/144] Testing {'criterion': 'squared_error', 'max_depth': 10, 'min_samples_leaf': 8, 'min_samples_split': 4}\n",
      "[36/144] Testing {'criterion': 'squared_error', 'max_depth': 10, 'min_samples_leaf': 8, 'min_samples_split': 8}\n",
      "[37/144] Testing {'criterion': 'friedman_mse', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "[38/144] Testing {'criterion': 'friedman_mse', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 4}\n",
      "[39/144] Testing {'criterion': 'friedman_mse', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 8}\n",
      "[40/144] Testing {'criterion': 'friedman_mse', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "[41/144] Testing {'criterion': 'friedman_mse', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 4}\n",
      "[42/144] Testing {'criterion': 'friedman_mse', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 8}\n",
      "[43/144] Testing {'criterion': 'friedman_mse', 'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "[44/144] Testing {'criterion': 'friedman_mse', 'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 4}\n",
      "[45/144] Testing {'criterion': 'friedman_mse', 'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 8}\n",
      "[46/144] Testing {'criterion': 'friedman_mse', 'max_depth': None, 'min_samples_leaf': 8, 'min_samples_split': 2}\n",
      "[47/144] Testing {'criterion': 'friedman_mse', 'max_depth': None, 'min_samples_leaf': 8, 'min_samples_split': 4}\n",
      "[48/144] Testing {'criterion': 'friedman_mse', 'max_depth': None, 'min_samples_leaf': 8, 'min_samples_split': 8}\n",
      "[49/144] Testing {'criterion': 'friedman_mse', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "[50/144] Testing {'criterion': 'friedman_mse', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 4}\n",
      "[51/144] Testing {'criterion': 'friedman_mse', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 8}\n",
      "[52/144] Testing {'criterion': 'friedman_mse', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "[53/144] Testing {'criterion': 'friedman_mse', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 4}\n",
      "[54/144] Testing {'criterion': 'friedman_mse', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 8}\n",
      "[55/144] Testing {'criterion': 'friedman_mse', 'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "[56/144] Testing {'criterion': 'friedman_mse', 'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 4}\n",
      "[57/144] Testing {'criterion': 'friedman_mse', 'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 8}\n",
      "[58/144] Testing {'criterion': 'friedman_mse', 'max_depth': 5, 'min_samples_leaf': 8, 'min_samples_split': 2}\n",
      "[59/144] Testing {'criterion': 'friedman_mse', 'max_depth': 5, 'min_samples_leaf': 8, 'min_samples_split': 4}\n",
      "[60/144] Testing {'criterion': 'friedman_mse', 'max_depth': 5, 'min_samples_leaf': 8, 'min_samples_split': 8}\n",
      "[61/144] Testing {'criterion': 'friedman_mse', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "[62/144] Testing {'criterion': 'friedman_mse', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 4}\n",
      "[63/144] Testing {'criterion': 'friedman_mse', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 8}\n",
      "[64/144] Testing {'criterion': 'friedman_mse', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "[65/144] Testing {'criterion': 'friedman_mse', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 4}\n",
      "[66/144] Testing {'criterion': 'friedman_mse', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 8}\n",
      "[67/144] Testing {'criterion': 'friedman_mse', 'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "[68/144] Testing {'criterion': 'friedman_mse', 'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 4}\n",
      "[69/144] Testing {'criterion': 'friedman_mse', 'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 8}\n",
      "[70/144] Testing {'criterion': 'friedman_mse', 'max_depth': 10, 'min_samples_leaf': 8, 'min_samples_split': 2}\n",
      "[71/144] Testing {'criterion': 'friedman_mse', 'max_depth': 10, 'min_samples_leaf': 8, 'min_samples_split': 4}\n",
      "[72/144] Testing {'criterion': 'friedman_mse', 'max_depth': 10, 'min_samples_leaf': 8, 'min_samples_split': 8}\n",
      "[73/144] Testing {'criterion': 'absolute_error', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "[74/144] Testing {'criterion': 'absolute_error', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 4}\n",
      "[75/144] Testing {'criterion': 'absolute_error', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 8}\n",
      "[76/144] Testing {'criterion': 'absolute_error', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "[77/144] Testing {'criterion': 'absolute_error', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 4}\n",
      "[78/144] Testing {'criterion': 'absolute_error', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 8}\n",
      "[79/144] Testing {'criterion': 'absolute_error', 'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "[80/144] Testing {'criterion': 'absolute_error', 'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 4}\n",
      "[81/144] Testing {'criterion': 'absolute_error', 'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 8}\n",
      "[82/144] Testing {'criterion': 'absolute_error', 'max_depth': None, 'min_samples_leaf': 8, 'min_samples_split': 2}\n",
      "[83/144] Testing {'criterion': 'absolute_error', 'max_depth': None, 'min_samples_leaf': 8, 'min_samples_split': 4}\n",
      "[84/144] Testing {'criterion': 'absolute_error', 'max_depth': None, 'min_samples_leaf': 8, 'min_samples_split': 8}\n",
      "[85/144] Testing {'criterion': 'absolute_error', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "[86/144] Testing {'criterion': 'absolute_error', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 4}\n",
      "[87/144] Testing {'criterion': 'absolute_error', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 8}\n",
      "[88/144] Testing {'criterion': 'absolute_error', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "[89/144] Testing {'criterion': 'absolute_error', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 4}\n",
      "[90/144] Testing {'criterion': 'absolute_error', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 8}\n",
      "[91/144] Testing {'criterion': 'absolute_error', 'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "[92/144] Testing {'criterion': 'absolute_error', 'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 4}\n",
      "[93/144] Testing {'criterion': 'absolute_error', 'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 8}\n",
      "[94/144] Testing {'criterion': 'absolute_error', 'max_depth': 5, 'min_samples_leaf': 8, 'min_samples_split': 2}\n",
      "[95/144] Testing {'criterion': 'absolute_error', 'max_depth': 5, 'min_samples_leaf': 8, 'min_samples_split': 4}\n",
      "[96/144] Testing {'criterion': 'absolute_error', 'max_depth': 5, 'min_samples_leaf': 8, 'min_samples_split': 8}\n",
      "[97/144] Testing {'criterion': 'absolute_error', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "[98/144] Testing {'criterion': 'absolute_error', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 4}\n",
      "[99/144] Testing {'criterion': 'absolute_error', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 8}\n",
      "[100/144] Testing {'criterion': 'absolute_error', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "[101/144] Testing {'criterion': 'absolute_error', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 4}\n",
      "[102/144] Testing {'criterion': 'absolute_error', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 8}\n",
      "[103/144] Testing {'criterion': 'absolute_error', 'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "[104/144] Testing {'criterion': 'absolute_error', 'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 4}\n",
      "[105/144] Testing {'criterion': 'absolute_error', 'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 8}\n",
      "[106/144] Testing {'criterion': 'absolute_error', 'max_depth': 10, 'min_samples_leaf': 8, 'min_samples_split': 2}\n",
      "[107/144] Testing {'criterion': 'absolute_error', 'max_depth': 10, 'min_samples_leaf': 8, 'min_samples_split': 4}\n",
      "[108/144] Testing {'criterion': 'absolute_error', 'max_depth': 10, 'min_samples_leaf': 8, 'min_samples_split': 8}\n",
      "[109/144] Testing {'criterion': 'poisson', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "[110/144] Testing {'criterion': 'poisson', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 4}\n",
      "[111/144] Testing {'criterion': 'poisson', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 8}\n",
      "[112/144] Testing {'criterion': 'poisson', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "[113/144] Testing {'criterion': 'poisson', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 4}\n",
      "[114/144] Testing {'criterion': 'poisson', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 8}\n",
      "[115/144] Testing {'criterion': 'poisson', 'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "[116/144] Testing {'criterion': 'poisson', 'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 4}\n",
      "[117/144] Testing {'criterion': 'poisson', 'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 8}\n",
      "[118/144] Testing {'criterion': 'poisson', 'max_depth': None, 'min_samples_leaf': 8, 'min_samples_split': 2}\n",
      "[119/144] Testing {'criterion': 'poisson', 'max_depth': None, 'min_samples_leaf': 8, 'min_samples_split': 4}\n",
      "[120/144] Testing {'criterion': 'poisson', 'max_depth': None, 'min_samples_leaf': 8, 'min_samples_split': 8}\n",
      "[121/144] Testing {'criterion': 'poisson', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "[122/144] Testing {'criterion': 'poisson', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 4}\n",
      "[123/144] Testing {'criterion': 'poisson', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 8}\n",
      "[124/144] Testing {'criterion': 'poisson', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "[125/144] Testing {'criterion': 'poisson', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 4}\n",
      "[126/144] Testing {'criterion': 'poisson', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 8}\n",
      "[127/144] Testing {'criterion': 'poisson', 'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "[128/144] Testing {'criterion': 'poisson', 'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 4}\n",
      "[129/144] Testing {'criterion': 'poisson', 'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 8}\n",
      "[130/144] Testing {'criterion': 'poisson', 'max_depth': 5, 'min_samples_leaf': 8, 'min_samples_split': 2}\n",
      "[131/144] Testing {'criterion': 'poisson', 'max_depth': 5, 'min_samples_leaf': 8, 'min_samples_split': 4}\n",
      "[132/144] Testing {'criterion': 'poisson', 'max_depth': 5, 'min_samples_leaf': 8, 'min_samples_split': 8}\n",
      "[133/144] Testing {'criterion': 'poisson', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "[134/144] Testing {'criterion': 'poisson', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 4}\n",
      "[135/144] Testing {'criterion': 'poisson', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 8}\n",
      "[136/144] Testing {'criterion': 'poisson', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "[137/144] Testing {'criterion': 'poisson', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 4}\n",
      "[138/144] Testing {'criterion': 'poisson', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 8}\n",
      "[139/144] Testing {'criterion': 'poisson', 'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "[140/144] Testing {'criterion': 'poisson', 'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 4}\n",
      "[141/144] Testing {'criterion': 'poisson', 'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 8}\n",
      "[142/144] Testing {'criterion': 'poisson', 'max_depth': 10, 'min_samples_leaf': 8, 'min_samples_split': 2}\n",
      "[143/144] Testing {'criterion': 'poisson', 'max_depth': 10, 'min_samples_leaf': 8, 'min_samples_split': 4}\n",
      "[144/144] Testing {'criterion': 'poisson', 'max_depth': 10, 'min_samples_leaf': 8, 'min_samples_split': 8}\n",
      "Best parameters found:  {'criterion': 'friedman_mse', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 4}\n",
      "Best MSE found:  0.048549534756431305\n",
      "0.7142857142857142\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data = load_wine()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Kreiramo regresor\n",
    "reg = DecisionTreeRegressor()\n",
    "\n",
    "param_grid = {\n",
    "    'criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'min_samples_split': [2, 4, 8],\n",
    "}\n",
    "\n",
    "# Inicijalizujemo objekat za pretragu hiperparametara upotrebom unakrsne validacije\n",
    "grid_search = ManualGridSearchCV(\n",
    "    reg,\n",
    "    param_grid,\n",
    "    scoring=lambda y_true, y_pred: -mean_squared_error(y_true, y_pred),\n",
    "    cv=5,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fitujemo grid - pretrazujemo prostor hiperparametara\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best MSE found: \", grid_search.best_score_ * -1)\n",
    "\n",
    "# Evaluiramo najbolji model na testnom skupu\n",
    "best_model = grid_search.best_estimator_\n",
    "test_prec = best_model.score(X_test, y_test)\n",
    "print(test_prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa0fc6b-e86e-42d9-825e-1d63057f4985",
   "metadata": {},
   "source": [
    "# Kompromisi\n",
    "\n",
    "Prilikom pretrage hiperparametara treba biti veoma pažljiv sa odabirom vrijednosti. Idealno bi bilo da se pretraži što veći skup vrijednosti međutim to značajno povećava vrijeme izvršavanja pa samim tim i račun za mašine koje to moraju da izvrše. Kako bi se rješio taj problem potrebno je identifikovati najbitnije parametre i za svaki od njih pronaći odgovarajući skup vrijednosti. Za dati izbor nema previše pravila i običnno podrazumijeva upotrebu iskustva i Googlanja kako bi se identifikovale bitne vrijednosti. Takođe obično je dobro početi sa velikim razmacima unutar vrijednosti za pretragu, pa zatim smanjivati dati opseg na manje intervale u zavisnosti od detektovanih performansi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22fab24-e6c0-4428-8d49-9037ff135cce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
