{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c1c74a9-0193-49e7-958b-268f4fe9725e",
   "metadata": {},
   "source": [
    "# Stabla odlučivanja (Decision Trees)\n",
    "\n",
    "Stabla odlučivanja predstavljaju strukturu veoma sličnu dijagramu toka (OSI). Sastavljeno je od 2 tipa čvorova:\n",
    "- međučvorovi - služe za donošenje odluke o daljoj putanji\n",
    "- listovi - kranji čvorovi na osnovu kojih generišemo predikciju našeg modela\n",
    "\n",
    "Stabla odlučivanja se mogu koristiti i za problem klasifikacije i regresije. U daljem tekstu glavni fokus će biti na klasifikaciji, dok se regresija ostavlja za vježbu.\n",
    "\n",
    "## Žašto mašinsko učenje a ne manuelno generisano stablo?\n",
    "\n",
    "Pošto je stablo odlučivanja struktura koja je veoma intuitivna postalvja se bitno pitanje. Zašto koristiti algoritam ako ga možemo sami generisati (posebno ako uzmemo u obzir da nijedan algoritam nije savršen i da uvijek postoji neka greška)? Pored problema velike količine podataka i kompleksnosti unutar istih, bitno je napomenuti još jedan problem koji predstavlja joše jedan od osnovnih razloga za upotrebu algoritama mašinskog učenja - promjenljivost domena. Promjenljivost domena podrazumijeva da se sami podaci i ponašanje pojava koje su za nas značajne mijenjaju sa vremenom, što obično znači da se pravila i mapiranja u okviru našeg modela isto moraju mijenjati kako bi njegove predikcije ostale relevantne. Ako posmatramo ponašanje kupaca, period godine značajno utiče na artikle koje će da kupuje (ljeto, zima, praznici...)\n",
    "\n",
    "## Algoritam za kreiranje stabla odlučivanja\n",
    "\n",
    "Osnovna ideja i dalje ostaje ista kao i kod svih drugih algoritama mašinskog učenja - minimizacija greške. Kod stabala odlučivanja u svakoj iteraciji algoritma treba da izaberemo jednu kolonu i jednu vrijednost na osnovu koje ćemo podijeliti naš dataset. Nakon podjele dobijamo dva nova podskupa našeg dataseta koji su disjunktni. Na jednoj strani imamo sve ulaze koji za datu kolonu imaju vrijednost manju od definisane, dok na drugoj strani imamo sve ulaze koji imaju vrijednosti veće od definisane (ilustracija na prezentaciji)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2a3fb3-4d03-492a-a496-81737baa4171",
   "metadata": {},
   "source": [
    "### Greška podjele\n",
    "\n",
    "Prilikom podjele dobijamo dva nova manja dataseta, gdje oba sadrže određeni broj ulaza koji pripada nekoj od mogućih klasa. Konačna predikcija se generiše tako što se u krajnjem dataset-u (koji je nastao nakon niza podjela) prebroji koja klasa se najčešće pojavljuje. Kako bi naša predikcija imala veću sigurnost (veća vjerovatnoća pripadanja datoj klasi) poželjno je da većina ulaza unutar posljednjeg dataset-a pripada jednoj klasi, odnosno da dati dataset bude homogen u odnosu na klasu koja se predviđa. \n",
    "\n",
    "Na osnovu date ideje se izvodi i greška koja se najčešće koristi za stabla odlučivanja a to je Gini Index. Gini Index je nastao kao mjera statističke disperzije za potrebe određivanja nejednakosti u primanjima. Kako bismo izračunali Gini Index cijele podjele potrebno je da izračunamo Gini Index jednog splita, odnosno jednog podskupa koji je dobijen nakon podjele:\n",
    "$$\n",
    "    Gini = 1 - \\sum_j{p_j^2}\n",
    "$$\n",
    "$p_j$ predstavlja vjerovatnoću pojavljivanja date klase u podskupu. Mоžemo primijetiti da manja vrijednost Gini indexa za split ukazuje da imamo homogeni split. Ukupan Gini index za cijeli split se dobija traženjem težinske srednje vrijednosti Gini indexa oba splita, gdje težina predstavlja dio dataseta koji se nalazi u podskupu:\n",
    "$$\n",
    "    TotalGini = \\frac{\\text{num of rows in left split}}{\\text{num of total rows}}* Gini_{left} + \\frac{\\text{num of rows in right split}}{\\text{num of total rows}}* Gini_{right}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e781037-9ebd-40de-bf25-df3059dbe112",
   "metadata": {},
   "source": [
    "### Pronalaženje uslova grananja\n",
    "\n",
    "Nakon što smo odredili koja je greška, potrebno je naći kombinaciju kolone i vrijednosti koja generiše dva nova splita tako da je greška (ukupan Gini index) minimalna. Najbolju vrijednost ćemo pronaću ako prođemo kroz sve moguće kombinacije parova (kolona, vrijednost) i pronađemo onaj par koji ima minimalan Gini index. U narednim iteracijama trebamo da gledamo sve postojeće splitove i da za svaki od tih splitova provjerimo sve nove kombinacije i da na kraju izaberemo kombinaciju u onom splitu koja ima ukupno najmanju grešku. \n",
    "\n",
    "Jako je bitno da odredimo u kom čvoru ćemo uopšte da napravimo split. Kako bi se izbjegao dodatan uslov pretrage, obično se definiše strategija rasta stabla:\n",
    "- po dubini (depth-first, DFS) - spuštamo se po dubini dok se ne ispuni neki uslov zaustavljanja\n",
    "- po širini (breadth-first, BFS) - idemo po nivoima dok se ne ispuni uslov zaustavljanja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8367f41-576d-4924-93a9-d92979e7305a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, max_depth=5, min_size=10):\n",
    "        \"\"\"\n",
    "        Inicijalizacija klasifikatora odluka stabla.\n",
    "        :param max_depth: Maksimalna dubina stabla.\n",
    "        :param min_size: Minimalan broj uzoraka potreban za podelu čvora.\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth  # ograničenje maksimalne dubine\n",
    "        self.min_size = min_size    # minimalan broj uzoraka za podelu\n",
    "        self.root = None            # korjenski čvor stabla\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Gradi stablo odluke na osnovu skupa za treniranje (X, y).\n",
    "        :param X: Lista ulaznih vektora.\n",
    "        :param y: Lista ciljnih klasa.\n",
    "        \"\"\"\n",
    "        # Kombinujemo X i y u jedinstveni dataset (poslednji element je labela)\n",
    "        dataset = [row[:] + [label] for row, label in zip(X, y)]\n",
    "        # Građenje stabla počevši od korena na dubini 1\n",
    "        self.root = self._build_tree(dataset, depth=1)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predviđa klase za ulazne podatke X.\n",
    "        :param X: Lista ulaznih vektora.\n",
    "        :return: Lista predviđenih klasa.\n",
    "        \"\"\"\n",
    "        # Rekurzivno prolazi kroz stablo za svaki red (vector)\n",
    "        return [self._predict_row(self.root, row) for row in X]\n",
    "\n",
    "    # ----- Pomoćne metode ----- #\n",
    "\n",
    "    def _gini_index(self, groups, class_values):\n",
    "        \"\"\"\n",
    "        Računa Gini nečistoću za podjelu.\n",
    "        :param groups: Lista grupa [levo, desno] nakon podjele.\n",
    "        :param class_values: Lista svih mogućih klasa.\n",
    "        :return: Gini indeks.\n",
    "        \"\"\"\n",
    "        n_instances = float(sum(len(group) for group in groups))  # ukupan broj primera\n",
    "        gini = 0.0\n",
    "        for group in groups:\n",
    "            size = float(len(group))\n",
    "            if size == 0:\n",
    "                continue  # izbegavamo deljenje nulom\n",
    "            score = 0.0\n",
    "            for class_val in class_values:\n",
    "                p = [row[-1] for row in group].count(class_val) / size\n",
    "                score += p * p\n",
    "            gini += (1.0 - score) * (size / n_instances)\n",
    "        return gini\n",
    "\n",
    "    def _test_split(self, index, value, dataset):\n",
    "        \"\"\"\n",
    "        Dijeli dataset na lijevi i desni podskup po pravilu: row[index] < value.\n",
    "        :param index: Indeks karakteristike po kojoj dijelimo podatke.\n",
    "        :param value: Vrijednost granice podjele.\n",
    "        :param dataset: Skup podataka (redovi sa labelama).\n",
    "        :return: Tuple (levo, desno) lista redova.\n",
    "        \"\"\"\n",
    "        left, right = [], []\n",
    "        for row in dataset:\n",
    "            if row[index] < value:\n",
    "                left.append(row)\n",
    "            else:\n",
    "                right.append(row)\n",
    "        return left, right\n",
    "\n",
    "    def _get_best_split(self, dataset):\n",
    "        \"\"\"\n",
    "        Pronalazi najbolju podjelu skupa podataka minimizujući Gini index.\n",
    "        :param dataset: Skup podataka koji treba da se podijele.\n",
    "        :return: Dictionary sa ključevima 'index', 'value', 'groups'.\n",
    "        \"\"\"\n",
    "        class_values = list(set(row[-1] for row in dataset))\n",
    "        best_index, best_value, best_score, best_groups = None, None, float('inf'), None\n",
    "        for index in range(len(dataset[0]) - 1):\n",
    "            for row in dataset:\n",
    "                groups = self._test_split(index, row[index], dataset)\n",
    "                gini = self._gini_index(groups, class_values)\n",
    "                if gini < best_score:\n",
    "                    best_index, best_value, best_score, best_groups = index, row[index], gini, groups\n",
    "        return {'index': best_index, 'value': best_value, 'groups': best_groups}\n",
    "\n",
    "    def _to_terminal(self, group):\n",
    "        \"\"\"\n",
    "        Kreira terminalni čvor vraćajući najčešću klasu u grupi.\n",
    "        :param group: Lista redova u čvoru.\n",
    "        :return: Vrednost klase (labela).\n",
    "        \"\"\"\n",
    "        outcomes = [row[-1] for row in group]\n",
    "        return max(set(outcomes), key=outcomes.count)\n",
    "\n",
    "    def _split(self, node, depth):\n",
    "        \"\"\"\n",
    "        Rekurzivno dijeli čvor sve dok se ne zadovolje uslovi zaustavljanja.\n",
    "        :param node: Trenutni čvor: dictionary sa ključevima 'index', 'value', 'groups'.\n",
    "        :param depth: Trenutna dubina stabla.\n",
    "        \"\"\"\n",
    "        left, right = node['groups']\n",
    "        del node['groups']\n",
    "        if not left or not right:\n",
    "            terminal = self._to_terminal(left + right)\n",
    "            node['left'] = node['right'] = terminal\n",
    "            return\n",
    "        if depth >= self.max_depth:\n",
    "            node['left'], node['right'] = self._to_terminal(left), self._to_terminal(right)\n",
    "            return\n",
    "        if len(left) <= self.min_size:\n",
    "            node['left'] = self._to_terminal(left)\n",
    "        else:\n",
    "            node['left'] = self._get_best_split(left)\n",
    "            self._split(node['left'], depth + 1)\n",
    "        if len(right) <= self.min_size:\n",
    "            node['right'] = self._to_terminal(right)\n",
    "        else:\n",
    "            node['right'] = self._get_best_split(right)\n",
    "            self._split(node['right'], depth + 1)\n",
    "\n",
    "    def _build_tree(self, train, depth):\n",
    "        \"\"\"\n",
    "        Počinje pravljenje stabla od korena.\n",
    "        :param train: Dataset za treniranje.\n",
    "        :param depth: Početna dubina (1).\n",
    "        :return: Rečnik koji predstavlja čvor korena.\n",
    "        \"\"\"\n",
    "        root = self._get_best_split(train)\n",
    "        self._split(root, depth)\n",
    "        return root\n",
    "\n",
    "    def _predict_row(self, node, row):\n",
    "        \"\"\"\n",
    "        Predviđa labelu za jedan ulazni vektor rekurzivno prolazeći stablo.\n",
    "        :param node: Trenutni čvor stabla ili terminalna vrednost.\n",
    "        :param row: Ulazni vektor.\n",
    "        :return: Predviđena labela.\n",
    "        \"\"\"\n",
    "        if isinstance(node, dict):\n",
    "            if row[node['index']] < node['value']:\n",
    "                return self._predict_row(node['left'], row)\n",
    "            else:\n",
    "                return self._predict_row(node['right'], row)\n",
    "        return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db858d7c-f774-4ded-8553-447e948daf2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tačnost na Iris test skupu: 100.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data.tolist(), iris.target.tolist()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=4, min_size=5)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dt.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Tačnost na Iris test skupu: {acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6ab573-6696-4f2d-ab64-bbd179eb3fc9",
   "metadata": {},
   "source": [
    "#### Heuristike za pronalaženje uslova\n",
    "\n",
    "Pretraga cijelog prostora svih mogućih kombinacija je jako spora, jer datasetovi mogu da imaju po nekoliko miliona redova i nekoliko stotina kolona. Zbog toga potrebno je naći način da se broj i redova i kolona redukuje kako bi algoritam mogao da se završi u razumnom vremenu. Dvije osnovne tehnike su:\n",
    "- Redukovani kandidatski splitovi\n",
    "    1. pronađemo sve moguće vrijednosti kolone i sortiramo ih\n",
    "    2. odaberemo određeni podskup vrijednosti koje se nalaze na istoj distanci\n",
    "pronađemo sve moguće vrijednosti kolone i sortiramo ih\n",
    "odaberemo određeni podskup vrijednosti koje se nalaze na istoj distanci\n",
    "- Radnom subspace sampling - biramo radnom podskup kolona pri svakoj iteraciji čije ćemo vrijednost gledati za split (ako se algoritam izvršava dovoljan broj iteracija sve kolone će biti razmotrene)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888f3ae4-5875-45c1-bec4-8aeff1555ea1",
   "metadata": {},
   "source": [
    "#### Spriječavanje overfitting-a\n",
    "\n",
    "S obzirom da u svakoj iteraciji kreiramo novi split koji ponovo dijeli prethodni split, algoritam je veoma podložan overfitting-u. U teoriji nakon dovoljnog broja iteracija možemo postići stanje gdje se u svakom listu nalazi samo jedan ulaz (potpuni overfitting). Kako bismo spriječili overfitting koristimo dva tipa pristupa:\n",
    "- Early stopping - podrazumijeva spriječavanje rada algoritma nakon što se postigao određeni broj iteracija, određena dubina, broj čvorova i slično. Veoma bitna tehnika koja se često koristi i kod većine drugih algoritama mašinskog učenja.\n",
    "- Prunning - razlikujemo dva tipa:\n",
    "    1. pre-prunning - slično early stoppingu, samo umjesto prekidanja rada algoritma odbacujemo trenutnu kombinaciju ako ona dovodi do stanja gdje se prelazi neka granična dubina, broj čvorova ili minimalni broj ulaza u splitu\n",
    "    2. post-prunning - primijenjuje se nakon što je generisan split da se provjeri koliko taj split doprinosi prediktivnoj moći našeg stabla. Jedan od osnovnih algoritama koji se koriste je Cost Complexity Prunning.\n",
    "    \n",
    "##### Cost Complexity Prunning (Weakest Link Pruning)\n",
    "\n",
    "Osnovna ideja je da izbacimo čvorove grananja koji ne doprinose previše krajnjoj predikciji i da ih zamijenimo sa listom. Posmatrajmo prvo kako bismo intuitivno odlučili da li neka odluka doprinosi na dobar način krajnjoj predikciji upotrebom Gini Index-a. Ako posmatramo Gini index cijelog podstabla od datog čvora dobijamo koliko je dato podstablo homogeno odnosno heterogeno. Odnosno ako bismo zamijenili cijelo podstablo listom, dobijamo koja je sigurnost naše predikcije. Ukoliko je cijelo podstablo recimo heterogeno (visoka vrijednost gini indexa) a dati čvor odluke generiše dva splita koji su homogeni (imaju nizak gini index) možemo reći da ta odluka utiče dobro na prediktivnu moć našeg stabla. Recimo ako imamo u jednom trenutku split sa 4 ulaza gdje 2 pripadaju klasi 1 a 2 pripadaju klasi 0. Ukoliko prolaskom kroz naše stablo dođemo do tog splita mi imamo vjerovatnoću od 50% da ulaz pripada klasi 0 ili 1, što je ekvivalentno da nasumično pogađamo. Ako na tom mjestu napravimo novo pravilo podjele koje dijeli split u dva nova splita gdje prvi ima samo 2 ulaza sa klasom 0, a drugi samo 2 ulaza sa klasom 1 onda dato pravilo dobro utiče na prediktivnu moć, jer prilikom dolaska do lista u datom podstablu možemo predvidjeti sa sigurnosti od 100% da ulaz pripada nekoj klasi. Dakle možemo zaključiti da je pravilo grananja korisno ako od heterogenog splita pravi dva nova splita koji su homogeni. Sa druge strane pravilo grananja nam nije korisno ako od heterogenog splita pravi nove heterogene splitove ili ako od već homogenog splita pravi nove homogene splitove.\n",
    "\n",
    "Formula za računanje je sljedeća:\n",
    "\n",
    "$$\\alpha=\\frac{R(t) - R(T_t)}{|{T_t} - 1|},$$\n",
    "\n",
    "gdje $T_t$ ukupan broj listova u podstablu, $R(t)$ Gini index ukoliko bi se podstablo zamijenilo sa jednim čvorom, dok $R(T_t)$ predstavlja sumu Gini indexa trenutnih listova. Faktor $\\alpha$ vidimo računa odnos između unaprjeđenja u Gini index-a i broja čvorova koji su bili potrebni da se to unaprijeđenje postigne. Izbor, granice za parametar $\\alpha$ za koju ćemo vršiti zamjenu podstabla jednim čvorom je izrazito bitan, i obično se definiše testiranjem više različitih vrijednosti i posmatranjem koja daje najbolju vrijednost tražene metrike (recall, precision, f1...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4611b8-cda1-4fca-aff3-79d536ae9ce2",
   "metadata": {},
   "source": [
    "## Ensemble learning\n",
    "\n",
    "Učenje u ansamblu predstavlja tehniku koja podrazumijeva korištenje kombinacije više različitih modela kako bi se generisala konačna predikcija. Ne odnosi se samo na stabla odlučivanja i može uključivati kombinaciju izlaza modela različitih tipova. Ideja je da je manja šansa da ukoliko više modela daje istu ili sličnu predikciju, postoji manja šansa da smo napravili grešku.\n",
    "\n",
    "Kod stabala odlučivanja razlikujemo dvije osnovne tehnike:\n",
    "- Bagging (Boostrap Aggregating)\n",
    "- Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac0d0a4-3572-4771-9dee-f3e82a217107",
   "metadata": {},
   "source": [
    "### Bagging \n",
    "\n",
    "Bagging je tehnika koja podrazumijeva treniranje više različitih modela istog tipa na podskupu podataka. Podskupe podataka kreiramo metodom pod nazivom Bootstrap Sampling. Bootstrap sampling kreira novi dataset tako što iz originalnog dataseta nasumično biramo ulaze. Sve ulaze biramo sa istom vjerovatnoćom i dozvoljeno je ponavljanje ulaza. Predikciju kreiramo tako što agregiramo rezultate svih modela. U slučaju klasifikacije izlazna klasa koja ima najveću vjerovatnoću (broj pojavljivanja date klase / broj modela). Bagging je pogodan zbog jednostavne paralelizacije proces treninga i generisanja predikcije (inferencije), jer ne postoji nikakva zavinost između modela. U kontekstu stabala odlučivanja Bagging tehnika je poznata i pod nazivom Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1259a7a8-28bf-4b21-9e97-4a692aa03f74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "\n",
    "class BaggingClassifier:\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_estimator,\n",
    "        n_estimators=10,\n",
    "        max_samples=1.0,\n",
    "        bootstrap=True,\n",
    "        random_state=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inicijalizacija Bagging klasifikatora.\n",
    "        :param base_estimator: Klasa baznog klasifikatora sa metodama fit(X, y) i predict(X).\n",
    "        :param n_estimators: Broj baznih modela u ansamblu.\n",
    "        :param max_samples: Procenat (ili broj) uzoraka za kreiranje podskupova podataka.\n",
    "        :param bootstrap: Da li uzorovati sa vraćanjem.\n",
    "        :param random_state: Seed za reproducibilnost.\n",
    "        \"\"\"\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_samples = max_samples\n",
    "        self.bootstrap = bootstrap\n",
    "        self.random_state = random_state\n",
    "        self.estimators_ = []\n",
    "\n",
    "        if random_state is not None:\n",
    "            random.seed(random_state)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Treniranje svih baznih modela na različitim bootstrap podskupovima.\n",
    "        :param X: Lista ulaznih vektora.\n",
    "        :param y: Lista klasa.\n",
    "        \"\"\"\n",
    "        n_samples = len(X)\n",
    "        # Određujemo broj uzoraka po estimatoru\n",
    "        if isinstance(self.max_samples, float) and 0 < self.max_samples <= 1:\n",
    "            k = int(self.max_samples * n_samples)\n",
    "        else:\n",
    "            k = int(self.max_samples)\n",
    "        for i in range(self.n_estimators):\n",
    "            # Kreiramo bootstrap uzorak\n",
    "            if self.bootstrap:\n",
    "                indices = [random.randrange(n_samples) for _ in range(k)]\n",
    "            else:\n",
    "                indices = random.sample(range(n_samples), k)\n",
    "            X_sample = [X[j] for j in indices]\n",
    "            y_sample = [y[j] for j in indices]\n",
    "\n",
    "            # Kopiramo i treniramo bazni estimator\n",
    "            estimator = copy.deepcopy(self.base_estimator)\n",
    "            estimator.fit(X_sample, y_sample)\n",
    "            self.estimators_.append(estimator)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predviđa ciljne klase glasanjem modela.\n",
    "        :param X: Lista ulaznih vektora.\n",
    "        :return: Lista predviđenih klasa.\n",
    "        \"\"\"\n",
    "        # Skupljanje predikcija svih estimator-a\n",
    "        predictions = [est.predict(X) for est in self.estimators_]\n",
    "        # Transponujemo listu da dobijemo predikcije po uzorku\n",
    "        # predictions: n_estimators x n_samples\n",
    "        n_samples = len(X)\n",
    "        final = []\n",
    "        for i in range(n_samples):\n",
    "            # Skup glasova za i-ti uzorak\n",
    "            votes = [predictions[j][i] for j in range(self.n_estimators)]\n",
    "            # Najčešći glas\n",
    "            final.append(max(set(votes), key=votes.count))\n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "006cb084-c9ac-4c52-9390-5918ff44d37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tačnost: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data.tolist(), iris.target.tolist()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "bag = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=5), n_estimators=20, max_samples=0.8, random_state=1)\n",
    "bag.fit(X_train, y_train)\n",
    "y_pred = bag.predict(X_test)\n",
    "print(f\"Tačnost: {sum(1 for i,j in zip(y_pred,y_test) if i==j)/len(y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af06ccbc-6696-4604-8224-82a12456fdb4",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "\n",
    "Boosting takođe uključuje treniranje više baznih modela. Glavna razlika u odnosu na Bagging je što postoji zavisnost između modela koje treniramo. Osnovna ideja je da se svaki novi model fokusira na unaprijeđivanje performansi na ulazima na kojima je prethodni model griješio. Postoji veliki broj različitih algoritama koji se mogu koristiti u ovu svrhu. U ostatku teksta mi ćemo se detaljnije fokusirati na AdaBoost algoritam.\n",
    "\n",
    "#### AdaBoost \n",
    "AdaBoost algoritam se zasniva na dodjeljivanju težina trening instancama. Samim tim greška svakog pojedničanog modela zavisi od težina. Takođe, pri određivanju konačne predikcije modeli koji imaju najbolje performanse (najmanju težinsku grešku) imaju najveći značaj. Na početku sve trening instance (svi ulazi) dobijaju istu težinu $1/N$. Nakon dodjeljivanja težina treniramo prvi bazni klasifikator (stablo odlučivanja) i računamo grešku tog klasifikatora. Bitna razlika u odnosu na prethodne slučajeve što formulu greške moramo modifikovati kako bismo uzeli u obzir težine svakog ulaza:\n",
    "$$\n",
    "    error = \\frac{\\sum_{i=1}^N{w_i * (p_i \\neq y_i)}}{\\sum_{i=1}^N{w_i}},\n",
    "$$\n",
    "gdje $p_i$ predstavlja predikciju klasifikatora, $y_i$ stvarnu klasu, a $w_i$ težinu datog ulaza. Možemo vidjeti da će greška klasifikatora direktno zavisiti od težina ulaza koji su pogrešno klasifikovani, dok ćemo tačno klasifikovane instance ignorisati. Bitno je napomenuti da će greška uvijek biti između 0 i 1, zbog normalizacije sa sumom svih težina. Nakon što smo dobili grešku potrebno je da izračunamo faktor $\\alpha$:\n",
    "$$\n",
    "    \\alpha = \\ln{\\frac{1 - error}{error}}\n",
    "$$\n",
    "Faktor $\\alpha$ predstavlja značaj trenutnog klasifikatora. Ukoliko klasifikator ima malen error rate, to znači da ima dobre performanse, te da bismo njegov izlaz trebali više da vrednujemo pri generisanju konačne predikcije. Takođe faktor alpha se koristi da promijenimo težine ulaza. Ulazi koji su bili tačno klasifikovani će dobiti manju težinu, dok ulazi koji su bili pogrešno klasifikovani dobiti veću težinu kako bi se više fokusirali na njih. Bitno je napomenuti da težine ne utiču direktno na fokusiranje na te izlaze. Za razliku od Bagginga ne pravimo poseban podskup podataka, nego i dalje koristimo cijeli dataset. Težine indikretno preko greške utiču da dodjeljujemo veći značaj modelima koji imaju dobre performanse na datim bitnim izlazima. Težine se mijenjaju na sljedeći način:\n",
    "- tačno klasifikovani ulazi - $w_i = w_i * e^{-\\alpha}$\n",
    "- pogrešno klasifikovani ulazi - $w_i = w_i * e^{\\alpha}$\n",
    "\n",
    "Obično se težine na kraju dodatno normalizuju tako da njihova suma iznosi 1. Konačna predikcija se generiše tako što nađemo težinsku sumu slabih modela, gdje je težina definisana faktorom $\\alpha$. \n",
    "\n",
    "U slučaju višeklasne klasifikacije potrebna je minimalna promjena korištene formule. U slučaju tačno klasifikovanih vrijednosti, ne vršimo modifikaciju težina (težina ostaje ista), dok pogrešnko klasifikovane i dalje mijenjamo na isti uz malu modifikaciju računanja faktora $\\alpha$':\n",
    "$$\n",
    "    \\alpha = \\ln{\\frac{1 - error}{error}} + \\ln{K-1},\n",
    "$$\n",
    "gdje se dodatni faktor $\\ln{K-1}$ dodaje kako bi $\\alpha$ i dalje ostao pozitivan u slučaju klasifikatora koji radi slučajno pogađanje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b320f0f0-6b53-4d8c-8aac-47398ded65b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import copy\n",
    "\n",
    "class AdaBoostClassifier:\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_estimator,\n",
    "        n_estimators=50,\n",
    "        random_state=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inicijalizacija AdaBoost klasifikatora.\n",
    "        :param base_estimator: Klasa baznog klasifikatora sa metodama fit(X, y) i predict(X).\n",
    "        :param n_estimators: Maksimalan broj baznih klasifikatora u ansamblu.\n",
    "        :param random_state: Seed za reproducibilnost.\n",
    "        \"\"\"\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.random_state = random_state\n",
    "        self.estimators_ = []         # lista treniranih modela\n",
    "        self.estimator_weights_ = []  # alfa vrednosti\n",
    "        self.classes_ = None          # lista svih klasa\n",
    "\n",
    "        if random_state is not None:\n",
    "            random.seed(random_state)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Treniranje AdaBoost za podatke sa proizvoljnim brojem klasa.\n",
    "        :param X: Lista ulaznih vektora.\n",
    "        :param y: Lista labela.\n",
    "        \"\"\"\n",
    "        n_samples = len(X)\n",
    "        # identifikujemo sve klase\n",
    "        self.classes_ = list(set(y))\n",
    "        K = len(self.classes_)\n",
    "        # inicijalne težine jednakim rasporedom\n",
    "        w = [1.0 / n_samples] * n_samples\n",
    "\n",
    "        for m in range(self.n_estimators):\n",
    "            # bootstrap uzorak po težinama\n",
    "            sample_indices = random.choices(range(n_samples), weights=w, k=n_samples)\n",
    "            X_sample = [X[i] for i in sample_indices]\n",
    "            y_sample = [y[i] for i in sample_indices]\n",
    "\n",
    "            # treniramo novi estimator\n",
    "            estimator = copy.deepcopy(self.base_estimator)\n",
    "            estimator.fit(X_sample, y_sample)\n",
    "\n",
    "            # predikcije na celom skupu\n",
    "            y_pred = estimator.predict(X)\n",
    "            # računamo težinsku grešku\n",
    "            error = sum(w[i] for i in range(n_samples) if y_pred[i] != y[i])\n",
    "            # prag za prekid: ako je prevelika ili nema greške\n",
    "            if error <= 0 or error >= 1 - (1 / K):\n",
    "                break\n",
    "\n",
    "            # alfa posle formule\n",
    "            alpha = math.log((1 - error) / error) + math.log(K - 1)\n",
    "            self.estimators_.append(estimator)\n",
    "            self.estimator_weights_.append(alpha)\n",
    "\n",
    "            # ažuriranje težina\n",
    "            for i in range(n_samples):\n",
    "                if y_pred[i] == y[i]:\n",
    "                    # tačno klasifikovani, težina ostaje ista\n",
    "                    w[i] = w[i]\n",
    "                else:\n",
    "                    # pogrešno klasifikovani, povećavamo težinu\n",
    "                    w[i] = w[i] * math.exp(alpha)\n",
    "            # normalizacija\n",
    "            Z = sum(w)\n",
    "            w = [wi / Z for wi in w]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predviđa klase upotrebe težinskog glasanja.\n",
    "        :param X: Lista ulaznih vektora.\n",
    "        :return: Lista predviđenih labela.\n",
    "        \"\"\"\n",
    "        n_samples = len(X)\n",
    "        # inicijalizujemo glasove za svaku klasu\n",
    "        agg_scores = [ {c: 0.0 for c in self.classes_} for _ in range(n_samples) ]\n",
    "\n",
    "        for alpha, estimator in zip(self.estimator_weights_, self.estimators_):\n",
    "            preds = estimator.predict(X)\n",
    "            for i in range(n_samples):\n",
    "                agg_scores[i][preds[i]] += alpha\n",
    "\n",
    "        y_pred = [ max(scores.items(), key=lambda item: item[1])[0] for scores in agg_scores ]\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "391d4e06-45f0-4d23-8742-2e91f29b3fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost tačnost: 1.00\n"
     ]
    }
   ],
   "source": [
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "adaboost = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=20, random_state=1)\n",
    "adaboost.fit(X_train, y_train)\n",
    "y_pred = adaboost.predict(X_test)\n",
    "acc = sum(1 for i,j in zip(y_pred, y_test) if i == j) / len(y_test)\n",
    "print(f\"AdaBoost tačnost: {acc:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
